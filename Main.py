# -*- coding: utf-8 -*-
"""Untitled5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15FihJq1JWB26bUiA9s_tFVHSIiwvWUpF
"""

!pip install datasets faiss-cpu==1.12.0 chromadb==0.4.22 sentence-transformers==2.3.1

import faiss
import numpy as np
from copy import deepcopy
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

dataset = load_dataset("openai/openai_humaneval")

dataset = dataset.with_format("python")

data_prompt = list(map(str, dataset['test']["prompt"]))
data_id = dataset['test']['task_id']
data_sol = dataset['test']['canonical_solution']

from sentence_transformers import SentenceTransformer
model_id = 'sentence-transformers/all-MiniLM-L6-v2'
dim = 384

device = "cpu"

embedder  = SentenceTransformer(model_id, device=device)
encoded_prompts = embedder.encode(data_prompt, show_progress_bar=True)

norm_encoded_prompts = deepcopy(encoded_prompts)
faiss.normalize_L2(norm_encoded_prompts)
norm_encoded_prompts = np.asarray(norm_encoded_prompts, dtype="float32")

records = [
    {
        "id": data_id[i],
        "prompt": data_prompt[i],
        "solution": data_sol[i]
    }
    for i in range(len(data_id))
]

faiss_index = faiss.IndexFlatIP(dim)
faiss_index.add(norm_encoded_prompts)

def retrieved(query,k):
  query_embed = embedder.encode([query]).astype("float32")
  faiss.normalize_L2(query_embed)

  D, I = faiss_index.search(query_embed, k)

  return [records[i] for i in I[0]]


def build_prompt(new_task, retrieved_data):
  examples = []
  for r in retrieved_data:
    examples.append( f"### Task: {r['id']}\n{r['prompt']}\n### Reference Solution:\n{r['solution']}")
  examples_block =   "\n\n".join(examples)

  return f"""You are an expert Python programmer.
Study the examples and then solve the new task.
Output **only** valid Python code, nothing else. Do not include comments above the function unless in a docstring.

### Examples
{examples_block}

### New Task
{new_task}

### Your Python solution (start writing code immediately after this line):
```python
"""

device = "cuda" if torch.cuda.is_available() else "cpu"

tokenizer = AutoTokenizer.from_pretrained("deepseek-ai/deepseek-coder-1.3b-instruct", trust_remote_code=True)
gen_model = AutoModelForCausalLM.from_pretrained("deepseek-ai/deepseek-coder-1.3b-instruct", trust_remote_code=True).to(device)

def extract_first_code_block(text):
    # Keep only what's between the first ```python and the next ```
    if "```python" in text:
        text = text.split("```python", 1)[-1]
    if "```" in text:
        text = text.split("```", 1)[0]
    return text.strip()

def generate_code(prompt):

  inputs = tokenizer(prompt, return_tensors="pt").to(device)
  outputs = gen_model.generate(
        **inputs,
        max_new_tokens=200,
        do_sample=False,
        pad_token_id=tokenizer.eos_token_id,
        eos_token_id=tokenizer.eos_token_id
    )
  text = tokenizer.decode(outputs[0], skip_special_tokens=True)


  return extract_first_code_block(text)

user_query = input("Enter your coding task: ")
retrieved_prompt = retrieved(user_query, k=1)
final_prompt = build_prompt(user_query, retrieved_prompt)
print("\n--- Generated Code ---\n")
print(generate_code(final_prompt))